{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKXNjhiaxxUi"
   },
   "source": [
    "# Fundamentals of Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Homework 2\n",
    "### Due: October 26th, 2023 at 11:59PM\n",
    "\n",
    "### Name: Chris Kong\n",
    "### Email: zk2086@nyu.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "AVL473j7W0CB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Use the same dataset that was released with HW1\n",
    "data = pd.read_csv('FML2023_HW1_Dataset.csv')\n",
    "# Separate the features, target values, and feature names\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oD2ESfcW7ai7"
   },
   "source": [
    "### Question 1: Maximum Likelihood Estimation (MLE) vs Maximum A Posteriori (MAP) (25 points)\n",
    "\n",
    "In Homework 1, we performed linear and ridge regression. To summarize:\n",
    "\n",
    "In Linear regression,\n",
    "\n",
    "$$\\beta = \\arg\\min_{\\beta}\\sum\\left(y_i - \\left(\\beta_0 + \\beta_1 x_{1i} +, \\ldots, + \\beta_px_{p i}\\right)\\right)^2$$\n",
    "\n",
    "\n",
    "* $J(\\beta)$ is the cost function.\n",
    "* $\\beta_0,\\ldots,\\beta_p$ are the coefficients for the features.\n",
    "* $x_{1i}$ represents the values of the feature for the i-th observation.\n",
    "* $y_i$ is the target value for the i-th observation.\n",
    "\n",
    "For ridge regression\n",
    "\n",
    "$$J(\\beta) = \\sum\\left(y_i - \\left(\\beta_0 + \\beta_1 x_{1i} +, \\ldots, + \\beta_px_{p i}\\right)\\right)^2 + \\lambda \\cdot \\sum \\beta_i^2$$\n",
    "\n",
    "* $\\lambda$ is the regularization hyper-parameter.\n",
    "\n",
    "**Task 1.1 (5 points)** Linear regression embodies Maximum Likelihood Estimation (MLE). Show that a closed form expression is $$\\beta = (\\mathbf{A}^\\top \\mathbf{A})^{-1}\\mathbf{A}^\\top \\mathbf{Y}$$ where $\\mathbf{A} = [X_1,\\ldots,X_n]$ and $\\mathbf{Y} = [Y_1,\\ldots,Y_n]$.\n",
    "\n",
    "**Task 1.2 (5 points)**: Ridge regression embodies Maximum A Posteriori (MAP), wherein the regularizer serves as the prior. Show that a closed form expression for the ridge estimator is $$\\beta = (\\mathbf{A}^\\top \\mathbf{A} + \\lambda I)^{-1}\\mathbf{A}^\\top \\mathbf{Y}$$ where $\\mathbf{A} = [X_1,\\ldots,X_n]$ and $\\mathbf{Y} = [Y_1,\\ldots,Y_n]$.\n",
    "\n",
    "**Task 1.3 Implementation (10 points):** Fill in the code below to differentiate between MLE and MAP.\n",
    "\n",
    "**Task 1.4 (5 points):**\n",
    "* Do MLE and MAP yield distinct solutions as the sample size tends to infinity? Explain your answer.\n",
    "\n",
    "* Will the impact of prior be greater with a small or large sample size, and what is the underlying rationale for this phenomenon?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1**\n",
    "\n",
    "\n",
    "Given a simple linear regression model, we have:\n",
    "\n",
    "$$  Y_i = \\beta^T X_i + \\epsilon_i $$\n",
    "\n",
    "Where $ \\epsilon_i $ is an error term. Assuming that errors are normally distributed with mean 0 and variance $\\sigma^2 $ $i.e.,  \\epsilon_i \\sim N(0, \\sigma^2) $, the likelihood function for observing the data is:\n",
    "\n",
    "$$ L(\\beta) = \\prod_{i=1}^n p(Y_i | X_i; \\beta) $$\n",
    "\n",
    "Given our assumption about the error term, this is:\n",
    "\n",
    "$$ L(\\beta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(Y_i - \\beta^T X_i)^2}{2\\sigma^2} \\right) $$\n",
    "\n",
    "To find the MLE of $ \\beta $, we want to maximize this likelihood. It's more convenient to maximize the log-likelihood, which is:\n",
    "\n",
    "$$ l(\\beta) = \\sum_{i=1}^n \\left[ -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(Y_i - \\beta^T X_i)^2}{2\\sigma^2} \\right] $$\n",
    "\n",
    "Setting the derivative of $ \\beta $ with respect to $ \\beta $ to zero, we get the normal equations:\n",
    "\n",
    "$$ \\mathbf{A}^\\top \\mathbf{A} \\beta = \\mathbf{A}^\\top \\mathbf{Y} $$\n",
    "\n",
    "Solving for $ \\beta $, we get:\n",
    "\n",
    "$$ \\beta = (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{Y} $$\n",
    "\n",
    "This is the closed-form solution for MLE in linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2**\n",
    "\n",
    "Ridge regression adds a penalty to the standard linear regression cost function. This penalty is on the magnitude of the coefficients. It's equivalent to imposing a Gaussian prior on the coefficients $ \\beta $. The objective function for ridge regression is:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\sum_{i=1}^n (Y_i - \\beta^T X_i)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\lambda $ is the regularization parameter.\n",
    "- $ p $ is the number of features.\n",
    "\n",
    "The likelihood for our data given this model is the same as for standard linear regression:\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_{i=1}^n p(Y_i | X_i; \\beta)\n",
    "$$\n",
    "\n",
    "But now, with our Gaussian prior on \\( \\beta \\), the posterior distribution is:\n",
    "\n",
    "$$\n",
    "P(\\beta | X, Y) \\propto L(\\beta) \\times P(\\beta)\n",
    "$$\n",
    "\n",
    "Where $ P(\\beta) $ is our Gaussian prior on the coefficients. The MAP estimate of $ \\beta $ is the value that maximizes this posterior distribution.\n",
    "\n",
    "Differentiating the ridge regression objective function with respect to \\( \\beta \\) and setting it to zero, we get the ridge regression normal equations:\n",
    "\n",
    "$$\n",
    "(\\mathbf{A}^\\top \\mathbf{A} + \\lambda I) \\beta = \\mathbf{A}^\\top \\mathbf{Y}\n",
    "$$\n",
    "\n",
    "Solving for \\( \\beta \\), we get the closed-form solution for MAP in ridge regression:\n",
    "\n",
    "$$\n",
    "\\beta = (\\mathbf{A}^\\top \\mathbf{A} + \\lambda I)^{-1} \\mathbf{A}^\\top \\mathbf{Y}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "oleMgs0V7Zsl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2900.193628493481"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mle_linear_regression(X, y):\n",
    "    # Augmenting X with a column of ones for the intercept term\n",
    "    X_aug = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "    \n",
    "    # Compute the MLE estimates using closed-form solution\n",
    "    theta_mle = np.linalg.inv(X_aug.T @ X_aug) @ X_aug.T @ y\n",
    "    return theta_mle\n",
    "\n",
    "# Calculate MLE estimates without bias\n",
    "theta_mle = mle_linear_regression(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "X_test_aug = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "y_preds_mle = X_test_aug @ theta_mle\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_mle = np.mean((y_test - y_preds_mle)**2)\n",
    "mse_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "cqvfD-8uF-Jl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2882.3289155849848"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_linear_regression(X, y, lambda_reg):\n",
    "    # Augmenting X with a column of ones for the intercept term\n",
    "    X_aug = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "    \n",
    "    # Compute the MAP estimates using closed-form solution with L2 regularization\n",
    "    identity_matrix = np.eye(X_aug.shape[1])\n",
    "    theta_map = np.linalg.inv(X_aug.T @ X_aug + lambda_reg * identity_matrix) @ X_aug.T @ y\n",
    "    return theta_map\n",
    "\n",
    "# Set the regularization parameter (lambda)\n",
    "lambda_reg = 0.01\n",
    "theta_map = map_linear_regression(X_train, y_train, lambda_reg)\n",
    "\n",
    "# Make predictions on the test set using MAP\n",
    "y_preds_map = X_test_aug @ theta_map\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for MAP\n",
    "mse_map = np.mean((y_test - y_preds_map)**2)\n",
    "mse_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4**\n",
    "\n",
    "**1. Do MLE and MAP yield distinct solutions as the sample size tends to infinity? Explain your answer.**\n",
    "\n",
    "As the sample size tends to infinity, the amount of data available becomes overwhelming and dominates any prior beliefs or regularization terms. In this scenario, the influence of the regularization term in MAP (ridge regression in this context) diminishes. Consequently, the MAP estimate approaches the MLE estimate. Therefore, as the sample size grows indefinitely, the solutions provided by MLE and MAP become indistinguishable.\n",
    "\n",
    "**2. Will the impact of the prior be greater with a small or large sample size, and what is the underlying rationale for this phenomenon?**\n",
    "\n",
    "The impact of the prior (regularization in ridge regression) is more pronounced when the sample size is small. When data is limited, the prior can play a significant role in shaping the solution, effectively acting as a form of bias that guides the model towards a particular solution. This can be especially useful in preventing overfitting when data is scarce. However, as the sample size grows and more data becomes available, the influence of the actual data on the model outweighs the influence of the prior. In essence, the data \"speaks louder\" than the prior, and the effect of the prior diminishes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwH8gND4XGvE"
   },
   "source": [
    "### Question 2: Classification with imbalanced dataset (20 points)\n",
    "\n",
    "We are creating an imbalanced version of the target variable for the Z dataset. An imbalanced dataset means that one class is much more frequent than the other class. In our case, we will consider the two classes as follows:\n",
    "\n",
    "- Class 0: Z progression values that are below the 75th percentile of the original target variable.\n",
    "- Class 1: Z progression values that are above the 75th percentile of the original target variable.\n",
    "\n",
    "By doing this, we are creating an imbalance where Class 0 will be more prevalent than Class 1, mimicking a common scenario in real-world imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "dQMA6OPsXD49"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Shuffle the data\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "\n",
    "# Create an imbalanced target variable\n",
    "y_imbalanced = np.where(y > np.percentile(y, 75), 1, 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_imbalanced, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJiy4EyUYX93"
   },
   "source": [
    "\n",
    "**Task 2.1 (3 points):**\n",
    "- Create a SVM classifier with a linear kernel, then calculate accuracy, precision, recall, and F1 score using available library functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "eKLZyXkwSDw0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8426966292134831, 0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 1. Create an SVM classifier with a linear kernel\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# 2. Fit the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make predictions on the test data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# 4. Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6mJ3w9TbWHT"
   },
   "source": [
    "\n",
    "**Task 2.2 (5 points):** What causes the metrics to exhibit lower values for the imbalanced dataset compared to those in homework 1?\n",
    "\n",
    "When dealing with imbalanced datasets, standard metrics such as accuracy can be misleading. Here's why:\n",
    "\n",
    "1. **Bias Towards the Majority Class**: Traditional classifiers, when trained on imbalanced datasets, tend to be biased towards the majority class. This is because the classifier will achieve a high accuracy by simply predicting the majority class for all inputs.\n",
    "\n",
    "2. **Misleading Accuracy**: In imbalanced datasets, even a naive model that always predicts the majority class can have a high accuracy. For instance, if 95% of the data belongs to Class 0 and 5% to Class 1, a model that always predicts Class 0 will have an accuracy of 95%.\n",
    "\n",
    "3. **Loss of Important Data**: The minority class, although less frequent, may represent critical data. In many real-world scenarios like fraud detection or rare disease diagnosis, the minority class is more important. If the model fails to classify the minority class correctly due to the imbalance, it can lead to significant consequences, even if the overall accuracy is high.\n",
    "\n",
    "4. **Performance Metrics**: Precision, recall, and F1 score can also be affected by class imbalance. For example, if the minority class (e.g., Class 1) is of interest, a low recall indicates many false negatives, meaning many actual positive instances are predicted as negative. This can be problematic in applications where detecting the positive class is crucial.\n",
    "\n",
    "To overcome these challenges and get a true sense of a model's performance on imbalanced datasets, it's essential to look beyond accuracy and consider other metrics like precision, recall, F1 score, and the area under the ROC curve (AUC-ROC). Additionally, techniques like resampling (oversampling or undersampling), using different evaluation metrics, or adopting algorithms that are designed to handle imbalance can be applied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPYn44O40aKX"
   },
   "source": [
    "**Random oversampling** is one of the many techniques used to address the class imbalance problem. It involves increasing the number of instances in the minority class by randomly duplicating existing instances. This helps to balance the class distribution and can lead to improved performance for certain models.\n",
    "\n",
    "**Task 2.3 (2 points):** Calculate and display the following statistics for the target variable (y) before applying random oversampling:\n",
    "  - Mean\n",
    "  - Standard Deviation\n",
    "  - Minimum\n",
    "  - Maximum\n",
    "\n",
    "**Task 2.4 (5 points):** Perform random oversampling on the training set. After oversampling, calculate and display the same statistics for the oversampled target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task2.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "PItrNXNsX7ou"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152.13348416289594, 77.00574586945044, 25.0, 346.0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate statistics for the target variable y (before applying random oversampling)\n",
    "mean_y = np.mean(y)\n",
    "std_y = np.std(y)\n",
    "min_y = np.min(y)\n",
    "max_y = np.max(y)\n",
    "\n",
    "mean_y, std_y, min_y, max_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task2.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5, 0, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# 1. Perform random oversampling on the training set\n",
    "X_train_oversampled, y_train_oversampled = resample(X_train[y_train == 1], y_train[y_train == 1], \n",
    "                                                    replace=True, n_samples=len(y_train[y_train == 0]), random_state=42)\n",
    "\n",
    "X_train_oversampled = np.vstack((X_train[y_train == 0], X_train_oversampled))\n",
    "y_train_oversampled = np.hstack((y_train[y_train == 0], y_train_oversampled))\n",
    "\n",
    "# 2. Calculate statistics for the oversampled target variable\n",
    "mean_y_oversampled = np.mean(y_train_oversampled)\n",
    "std_y_oversampled = np.std(y_train_oversampled)\n",
    "min_y_oversampled = np.min(y_train_oversampled)\n",
    "max_y_oversampled = np.max(y_train_oversampled)\n",
    "\n",
    "mean_y_oversampled, std_y_oversampled, min_y_oversampled, max_y_oversampled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oel3kCeV2tOM"
   },
   "source": [
    "**Task 2.5 (5 points):**\n",
    "- Create another instance of SVM classifier with linear kernel, fit it on the oversampled data and calculate all the prior metrics for the oversampled model.\n",
    "- Show the metrics with different regularization parameters {0.1, 1, 10, 100} on the linear kernel.\n",
    "- Show the metrics with polynomial degrees {-1, 0, 3, 4} and observe how the model's complexity changes.\n",
    "- Introduce different values for the regularization parameter in the RBF kernel and show how it balances the trade-off between maximizing the margin and minimizing classification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "LeictOez24d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7865168539325843,\n",
       " 0.41935483870967744,\n",
       " 0.9285714285714286,\n",
       " 0.5777777777777778)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function to calculate metrics for different SVM configurations\n",
    "def calculate_svm_metrics(X_train, y_train, X_test, y_test, kernel='linear', C=1.0, degree=3):\n",
    "    svm_classifier = SVC(kernel=kernel, C=C, degree=degree, random_state=42)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# 1. Create another instance of the SVM classifier with a linear kernel and fit it on the oversampled data\n",
    "# 2. Calculate accuracy, precision, recall, and F1 score for the oversampled model\n",
    "accuracy_oversampled, precision_oversampled, recall_oversampled, f1_oversampled = calculate_svm_metrics(\n",
    "    X_train_oversampled, y_train_oversampled, X_test, y_test)\n",
    "\n",
    "accuracy_oversampled, precision_oversampled, recall_oversampled, f1_oversampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.741573</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.530612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.786517</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.577778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.523810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  Precision    Recall  F1 Score\n",
       "0.1    0.741573   0.371429  0.928571  0.530612\n",
       "1.0    0.786517   0.419355  0.928571  0.577778\n",
       "10.0   0.775281   0.392857  0.785714  0.523810\n",
       "100.0  0.764045   0.379310  0.785714  0.511628"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regularization parameters to test\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "\n",
    "# Calculate metrics for different regularization parameters on the linear kernel\n",
    "metrics_results = []\n",
    "for C in C_values:\n",
    "    accuracy, precision, recall, f1 = calculate_svm_metrics(X_train_oversampled, y_train_oversampled, X_test, y_test, C=C)\n",
    "    metrics_results.append((accuracy, precision, recall, f1))\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_results, columns=['Accuracy', 'Precision', 'Recall', 'F1 Score'], index=C_values)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.157303</td>\n",
       "      <td>0.157303</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.271845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.347826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1 Score\n",
       "0  0.157303   0.157303  1.000000  0.271845\n",
       "1  0.764045   0.379310  0.785714  0.511628\n",
       "3  0.831461   0.473684  0.642857  0.545455\n",
       "4  0.831461   0.444444  0.285714  0.347826"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polynomial degrees to test\n",
    "degree_values = [0, 1, 3, 4]\n",
    "\n",
    "# Calculate metrics for different polynomial degrees\n",
    "poly_metrics_results = []\n",
    "for degree in degree_values:\n",
    "    accuracy, precision, recall, f1 = calculate_svm_metrics(X_train_oversampled, y_train_oversampled, X_test, y_test, kernel='poly', degree=degree)\n",
    "    poly_metrics_results.append((accuracy, precision, recall, f1))\n",
    "\n",
    "poly_metrics_df = pd.DataFrame(poly_metrics_results, columns=['Accuracy', 'Precision', 'Recall', 'F1 Score'], index=degree_values)\n",
    "poly_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.786517</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.540541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.484848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  Precision    Recall  F1 Score\n",
       "0.1    0.786517   0.413793  0.857143  0.558140\n",
       "1.0    0.808989   0.434783  0.714286  0.540541\n",
       "10.0   0.808989   0.421053  0.571429  0.484848\n",
       "100.0  0.764045   0.315789  0.428571  0.363636"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate metrics for different regularization parameters on the RBF kernel\n",
    "rbf_metrics_results = []\n",
    "for C in C_values: #We will reuse the C values for convenience\n",
    "    accuracy, precision, recall, f1 = calculate_svm_metrics(X_train_oversampled, y_train_oversampled, X_test, y_test, kernel='rbf', C=C)\n",
    "    rbf_metrics_results.append((accuracy, precision, recall, f1))\n",
    "\n",
    "rbf_metrics_df = pd.DataFrame(rbf_metrics_results, columns=['Accuracy', 'Precision', 'Recall', 'F1 Score'], index=C_values)\n",
    "rbf_metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oquatEvqroqa"
   },
   "source": [
    "### Question 3: Naive Bayes Model (10 points)\n",
    "\n",
    "Implement the Naieve Bayes classifer on the Z dataset. \n",
    "\n",
    "We will assume that each continuous feature $X_i$ of $X$ follow a Gaussian distribution within each class $Y$.\n",
    "\n",
    "- For each class $c$, calculate the mean $(\\mu_c)$ and standard deviation $(\\sigma_c)$ for each feature. These parameters represent the central tendency and spread of the feature values within each class. They can be computed as:\n",
    "\n",
    "   \\begin{align*}\n",
    "   \\mu_c^j &= \\frac{1}{N_c} \\sum_{i=1}^{N_c} X_i^j \\quad \\text{(mean of feature \\(j\\) in class \\(c\\))} \\\\\n",
    "   \\sigma_c^j &= \\sqrt{\\frac{1}{N_c} \\sum_{i=1}^{N_c} (X_i^j - \\mu_c^j)^2} + \\epsilon \\quad \\text{(standard deviation of feature \\(j\\) in class \\(c\\))}\n",
    "   \\end{align*}\n",
    "     \n",
    "   where $N_c$ is the number of data points in class $c$, and $\\varepsilon=1e^{-6}$ is a small constant added for numerical stability.\n",
    "\n",
    "- To make a prediction for a new data point $x$, calculate the probability of $x$ belonging to each class $c$ using the Gaussian probability density function:\n",
    "\n",
    "   \\begin{align*}\n",
    "   P(X^j = x^j | Y = c) = \\frac{1}{\\sqrt{2\\pi}\\sigma_c^j} e^{-\\frac{1}{2}\\left(\\frac{x^j - \\mu_c^j}{\\sigma_c^j}\\right)^2}\n",
    "   \\end{align*}\n",
    "\n",
    "- Calculate the class probability $P(Y = c | X = x)$ as the product of the probabilities of each feature:\n",
    "\n",
    "    \\begin{align*}\n",
    "     P(Y = c | X = x) = P(Y = c) \\prod_{j=1}^{D} P(X^j = x^j | Y = c)\n",
    "    \\end{align*}\n",
    "\n",
    "   where $D$ is the number of features.\n",
    "\n",
    "- Assign the class label to the class with the highest probability:\n",
    "\n",
    "    \\begin{align*}\n",
    "     \\hat{Y} = \\arg\\max_{c} P(Y = c | X = x)\n",
    "     \\end{align*}\n",
    "\n",
    "**Hint:** In the code for Gaussian Naive Bayes, we take logarithms in certain calculations. This is a common technique used to avoid numerical underflow, especially when working with small probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "JEIVwXWKrscC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7640449438202247"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define epsilon for numerical stability\n",
    "epsilon = 1e-6\n",
    "\n",
    "class GaussianNaiveBayes:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = {}\n",
    "        for c in self.classes:\n",
    "            data_c = X[y == c]\n",
    "            mu_c = np.mean(data_c, axis=0)\n",
    "            sigma_c = np.std(data_c, axis=0) + epsilon\n",
    "            self.parameters[c] = {'mean': mu_c, 'std': sigma_c}\n",
    "\n",
    "    def _calculate_likelihood(self, x, mean, std):\n",
    "        return (1.0 / (np.sqrt(2 * np.pi) * std)) * np.exp(-0.5 * ((x - mean) / std) ** 2)\n",
    "\n",
    "    def _calculate_class_probability(self, x, c):\n",
    "        p_y = len(y_train_oversampled[y_train_oversampled == c]) / len(y_train_oversampled)\n",
    "        p_x_given_y = np.prod([self._calculate_likelihood(x[j], self.parameters[c]['mean'][j], self.parameters[c]['std'][j]) for j in range(len(x))])\n",
    "        return p_y * p_x_given_y\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [max(self.classes, key=lambda c: self._calculate_class_probability(x, c)) for x in X]\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = np.mean(y_pred == y)\n",
    "        return accuracy\n",
    "\n",
    "# Initialize and train the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNaiveBayes()\n",
    "gnb.fit(X_train_oversampled, y_train_oversampled)\n",
    "y_pred = gnb.predict(X_test.values)\n",
    "accuracy = gnb.score(X_test.values, y_test)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011235955056179775"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the original dataset:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Use the same dataset that was released with HW1\n",
    "data = pd.read_csv('FML2023_HW1_Dataset.csv')\n",
    "# Separate the features, target values, and feature names\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define epsilon for numerical stability\n",
    "epsilon = 1e-3\n",
    "\n",
    "class GaussianNaiveBayes:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = {}\n",
    "        for c in self.classes:\n",
    "            data_c = X[y == c]\n",
    "            mu_c = np.mean(data_c, axis=0)\n",
    "            sigma_c = np.std(data_c, axis=0) + epsilon\n",
    "            self.parameters[c] = {'mean': mu_c, 'std': sigma_c}\n",
    "\n",
    "    def _calculate_likelihood(self, x, mean, std):\n",
    "        return (1.0 / (np.sqrt(2 * np.pi) * std)) * np.exp(-0.5 * ((x - mean) / std) ** 2)\n",
    "\n",
    "    def _calculate_class_probability(self, x, c):\n",
    "        p_y = len(y_train[y_train == c]) / len(y_train)\n",
    "        p_x_given_y = np.prod([self._calculate_likelihood(x[j], self.parameters[c]['mean'][j], self.parameters[c]['std'][j]) for j in range(len(x))])\n",
    "        return p_y * p_x_given_y\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [max(self.classes, key=lambda c: self._calculate_class_probability(x, c)) for x in X]\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = np.mean(y_pred == y)\n",
    "        return accuracy\n",
    "\n",
    "# Initialize and train the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNaiveBayes()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test.values)\n",
    "accuracy = gnb.score(X_test.values, y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4czskeTTCMam"
   },
   "source": [
    "### Question 4: ROC curve and AUROC (15 points)\n",
    "\n",
    "**Task 4.1 (3 points):** Imagine you are a public health researcher investigating the performance of a new diagnostic test for disease Z, which is a potentially life-threatening condition. The test is designed to identify individuals who have the disease. You have collected data from a group of 500 patients who were tested for disease Z, and the results are as follows:\n",
    "\n",
    "Out of 150 patients who actually have disease Z, the test correctly identified 120 of them as positive.\n",
    "However, the test also falsely identified 50 patients who do not have disease Z as positive.\n",
    "\n",
    "* **Precision:** Define precision in the context of this diagnostic test for disease Z. Calculate the precision of the test based on the provided data.\n",
    "* **Recall:** Explain what recall means in this scenario. Calculate the recall of the test based on the provided data.\n",
    "* **F1-score:** Define the F1-score and explain why it is important, especially in the context of diagnosing a serious disease like Z. Calculate the F1-score of the test based on the provided data.\n",
    "* **Specificity:** What is specificity, and why is it relevant when evaluating a diagnostic test like this one? Calculate the specificity of the test based on the provided data.\n",
    "* **Balanced Accuracy:** Describe what balanced accuracy is and why it might be a useful metric in this situation. Calculate the balanced accuracy of the test based on the provided data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Definitions and Calculations:\n",
    "\n",
    "1. **Precision (Positive Predictive Value)**:\n",
    "    - **Definition**: Precision indicates the proportion of positive identifications (by the test) that were actually correct. It's a measure of the test's accuracy when declaring a positive result.\n",
    "    - **Formula**: \n",
    "    $$\n",
    "    \\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Positives (FP)}}\n",
    "    $$\n",
    "    \n",
    "2. **Recall (Sensitivity or True Positive Rate)**:\n",
    "    - **Definition**: Recall measures the proportion of actual positives that were correctly identified by the test. It's a measure of the test's ability to find all the positive cases.\n",
    "    - **Formula**: \n",
    "    $$\n",
    "    \\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Negatives (FN)}}\n",
    "    $$\n",
    "\n",
    "3. **F1-score**:\n",
    "    - **Definition**: The F1-score is the harmonic mean of precision and recall. It provides a balance between the two. When diagnosing a serious disease, it's crucial because both false negatives and false positives have serious implications.\n",
    "    - **Formula**: \n",
    "    $$\n",
    "    \\text{F1-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "    $$\n",
    "\n",
    "4. **Specificity (True Negative Rate)**:\n",
    "    - **Definition**: Specificity measures the proportion of actual negatives that were correctly identified by the test. It's crucial because it tells us how good the test is at avoiding false alarms.\n",
    "    - **Formula**: \n",
    "    $$\n",
    "    \\text{Specificity} = \\frac{\\text{True Negatives (TN)}}{\\text{True Negatives (TN) + False Positives (FP)}}\n",
    "    $$\n",
    "\n",
    "5. **Balanced Accuracy**:\n",
    "    - **Definition**: Balanced accuracy is the average of recall and specificity. It provides a balanced measure of the test's performance across both positive and negative cases. In situations where there's a class imbalance or where both types of errors have significant implications, balanced accuracy can be a more informative metric.\n",
    "    - **Formula**: \n",
    "    $$\n",
    "    \\text{Balanced Accuracy} = \\frac{\\text{Recall} + \\text{Specificity}}{2}\n",
    "    $$\n",
    "\n",
    "From the data provided:\n",
    "- True Positives (TP) = 120\n",
    "- False Positives (FP) = 50\n",
    "- False Negatives (FN) = 150 - 120 = 30\n",
    "- True Negatives (TN) = 500 - 150 - 50 = 300\n",
    "\n",
    "Using the above values, we can calculate Precision, Recall, F1-score, Specificity, and Balanced Accuracy.\n",
    "\n",
    "Here are the calculated values for the metrics based on the provided data:\n",
    "\n",
    "1. **Precision**: Approximately \\(0.706\\) or \\(70.6\\%\\)\n",
    "2. **Recall**: \\(0.8\\) or \\(80\\%\\)\n",
    "3. **F1-score**: Approximately \\(0.750\\) or \\(75\\%\\)\n",
    "4. **Specificity**: Approximately \\(0.857\\) or \\(85.7\\%\\)\n",
    "5. **Balanced Accuracy**: Approximately \\(0.829\\) or \\(82.9\\%\\)\n",
    "\n",
    "### Interpretation:\n",
    "- **Precision**: Of all the patients the test identified as positive for disease Z, about \\(70.6\\%\\) actually had the disease.\n",
    "- **Recall**: The test was able to correctly identify \\(80\\%\\) of the patients who actually had disease Z.\n",
    "- **F1-score**: This score, being the harmonic mean of precision and recall, indicates a balanced measure of the test's accuracy and its ability to detect positive cases. It's at \\(75\\%\\), suggesting a relatively good balance between precision and recall.\n",
    "- **Specificity**: The test correctly identified approximately \\(85.7\\%\\) of the patients who did not have disease Z.\n",
    "- **Balanced Accuracy**: This metric gives an average accuracy across both positive and negative cases, and it's at \\(82.9\\%\\). This suggests that the test performs fairly well across both classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPOZo4QM5V_y"
   },
   "source": [
    "**Task 4.2 (6 Points)** Plot the ROC curve\n",
    "\n",
    "An ROC curve plots TPR (y-axis) vs. FPR (x-axis) at all classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives.\n",
    "\n",
    "See this for more details (https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "\n",
    "Plot the ROC curve for Disease Z HW1 dataset with SVM classifier. **Note that you are not allowed to use any library function to compute the ROC. You have to do it from scratch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "aZnZs_QC5oxQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  target  \n",
       "0 -0.002592  0.019907 -0.017646   151.0  \n",
       "1 -0.039493 -0.068332 -0.092204    75.0  \n",
       "2 -0.002592  0.002861 -0.025930   141.0  \n",
       "3  0.034309  0.022688 -0.009362   206.0  \n",
       "4 -0.002592 -0.031988 -0.046641   135.0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your code to compute and plot ROC goes here\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"FML2023_HW1_Dataset.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>binary_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>151.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>206.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  binary_target\n",
       "0   151.0              1\n",
       "1    75.0              0\n",
       "2   141.0              1\n",
       "3   206.0              1\n",
       "4   135.0              0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarize the target column based on its median\n",
    "threshold = data['target'].median()\n",
    "data['binary_target'] = (data['target'] > threshold).astype(int)\n",
    "\n",
    "# Display the first few rows of the dataset after binarization\n",
    "data[['target', 'binary_target']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True, random_state=42)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data.drop(columns=['target', 'binary_target'])\n",
    "y = data['binary_target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM classifier on the training set\n",
    "svm_classifier = SVC(probability=True, kernel='linear', random_state=42)\n",
    "svm_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Predict probabilities on the testing set\n",
    "y_probs = svm_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Initialize lists to store TPR and FPR values\n",
    "tpr_values = []\n",
    "fpr_values = []\n",
    "\n",
    "# Define a function to compute TPR and FPR\n",
    "def compute_tpr_fpr(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    TPR = TP / (TP + FN)\n",
    "    FPR = FP / (FP + TN)\n",
    "    \n",
    "    return TPR, FPR\n",
    "\n",
    "# Iterate over various thresholds to compute TPR and FPR\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "    tpr, fpr = compute_tpr_fpr(y_test, y_pred)\n",
    "    tpr_values.append(tpr)\n",
    "    fpr_values.append(fpr)\n",
    "\n",
    "# print values: (commented out for display clarity)\n",
    "# tpr_values, fpr_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_values, tpr_values, label='SVM Classifier', color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9sRjUMk592y"
   },
   "source": [
    "**Task 4.3 (6 Points):** Compute the AUC of ROC\n",
    "\n",
    "AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.\n",
    "\n",
    "Compute the AUC of your SVM model. **Note that you are not allowed to use any library function to compute the AUC. You have to do it from scratch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "DOBw8ACA587S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8385204081632653"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your code to compute the AUC goes here\n",
    "# Sort FPR and corresponding TPR values\n",
    "sorted_indices = np.argsort(fpr_values)\n",
    "sorted_fpr = np.array(fpr_values)[sorted_indices]\n",
    "sorted_tpr = np.array(tpr_values)[sorted_indices]\n",
    "\n",
    "# Compute the AUC using the trapezoidal rule with sorted values\n",
    "auc = 0\n",
    "for i in range(1, len(sorted_fpr)):\n",
    "    auc += (sorted_tpr[i] + sorted_tpr[i-1]) * (sorted_fpr[i] - sorted_fpr[i-1]) / 2\n",
    "\n",
    "auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
